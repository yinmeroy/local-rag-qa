# 本地知识库问答工具

一个基于Ollama+LangChain+FAISS的轻量级本地知识库问答系统，支持上传PDF/TXT文档并进行自然语言问答，界面简洁易用，适配Windows系统，支持多模型懒加载+按需下载。

## 功能特点
- **本地部署**：所有数据（文档、向量库）均存储在本地，无需联网，数据安全可控
- **简洁界面**：整合模型选择、文件上传、问答、缓存管理于同一界面，无需切换标签
- **自动优化**：上传新文件自动清空旧对话和缓存，回复统一为规范中文
- **轻量高效**：基于FAISS向量检索，支持大文档智能分块，避免内容碎片化
- **多格式支持**：兼容PDF/TXT文档，自动处理编码和格式问题
- **多模型切换**：支持懒加载+按需下载，无需提前下载所有模型，节省内存，支持模型一键切换
- **严格校验**：未下载模型时拦截上传/问答操作，避免无效报错，提示清晰

## 环境配置

### 1. 基础环境准备
- **Python版本**：推荐Python 3.8 ~ 3.11（3.12+可能存在依赖兼容问题）
- **系统要求**：Windows 10/11（64位）
- **硬件要求**：内存≥8GB（推荐16GB），磁盘空间≥10GB（用于存储模型和向量库）

### 2. 虚拟环境配置
```bash
# 创建虚拟环境（命名为py311_env）
python -m venv py311_env

# 激活环境（Windows）
py311_env\Scripts\activate

# 升级pip（避免依赖安装失败）
pip install --upgrade pip
```

### 3. 依赖安装
```bash
# 安装项目核心依赖
pip install -r requirements.txt
```

### 4. Ollama完整配置（核心步骤）
#### 4.1 安装Ollama
- 下载地址：[Ollama官方下载页](https://ollama.com/download)
- 验证安装：终端执行 `ollama --version` 显示版本号即成功

#### 4.2 启动 Ollama 服务
```bash
# 启动 Ollama 后台服务（必须保持运行，单独终端窗口执行）
ollama serve
```

#### 4.3 下载**必需模型**（按需下载，无需全部下载）
> **核心优化**：无需提前下载所有模型，仅需下载你要使用的 1 个大语言模型 + 1 个嵌入模型，节省内存
```bash
# 示例1：轻量化组合（推荐，总占用约2.1GB，适合内存不足场景）
# 1. 大语言模型（选其一即可）
ollama pull phi3  # 小体积模型，约2GB，中文友好
# ollama pull llama3  # 均衡型模型，约4GB，综合效果好
# ollama pull qwen    # 阿里云通义千问，约3GB，中文优化

# 2. 嵌入模型（必须下载一个）
ollama pull nomic-embed-text  # 轻量嵌入模型，约500MB（推荐）
# ollama pull all-minilm      # 超轻量嵌入模型，约100MB
```

#### 4.4 验证模型
```bash
# 测试已下载的大语言模型（返回正常回复即成功）
ollama run phi3 "你好"

# 测试已下载的嵌入模型（无报错即成功）
ollama run nomic-embed-text "测试文本"
```

## 项目结构
```
local_rag/
├── config/                # 配置文件目录
│   ├── settings.py        # 模型/路径/端口等核心配置
│   └── model_config.py    # 支持的模型列表配置（多模型切换核心）
├── models/                # Ollama客户端封装
│   └── ollama_client.py   # 对话/嵌入功能封装（懒加载+模型检测）
├── vector_db/             # FAISS向量库管理
│   └── faiss_manager.py   # 向量库创建/检索/清空（适配动态嵌入模型）
├── document/              # 文档处理模块
│   └── doc_processor.py   # PDF/TXT加载、分块、过滤（防碎片化）
├── ui/                    # 界面模块
│   └── gradio_ui.py       # Gradio界面逻辑（最终优化版）
├── main.py                # 项目启动入口（含Ollama连接校验）
└── requirements.txt       # 依赖清单（精准版本，避免兼容问题）
```

## 启动项目
```bash
# 确保虚拟环境已激活 + Ollama服务已启动（单独终端）
python main.py
```

启动成功后，浏览器访问：`http://127.0.0.1:7860`（默认端口7860）

## 使用说明
### 1. 模型初始化（第一步必做）
- 在界面顶部下拉框选择已下载的**大语言模型**和**嵌入模型**
- 点击「初始化模型」按钮，系统会自动检测模型是否已下载：
  - ✅ 提示“所有模型已就绪，可上传文件”：模型检测通过，可继续操作
  - ❌ 提示下载命令：复制命令到终端执行，下载完成后重新初始化
- 模型未初始化/未下载时，无法进行后续的上传和问答操作

### 2. 文档上传
- 点击「上传文件」按钮，选择PDF/TXT格式文档（单文件建议≤100MB）
- 上传后自动完成：文档加载 → 智能分块 → 向量嵌入 → 向量库构建
- 状态提示：
  - ✅ 上传成功，可开始提问：向量库构建完成，可正常问答
  - ❌ 上传失败：根据提示检查文档格式/大小/权限，或查看终端报错

### 3. 智能问答
- 在输入框中输入问题（如“文档的主要研究对象是什么？”）
- 按下回车，模型基于上传文档内容返回规范中文回答
- 支持多轮对话，模型会记住上下文，问答更精准

### 4. 缓存管理
- 点击「删除文件缓存」：清空已构建的向量库（用于切换文档或清理数据）
- 上传新文件时，会**自动清空旧对话+旧缓存**，无需手动操作
- 缓存文件存储在项目根目录的 `faiss_local_index` 文件夹

### 5. 多模型切换
- 如需切换模型，重新选择下拉框中的模型 → 点击「初始化模型」即可
- 模型用完后可删除释放内存：
  ```bash
  # 查看本地已下载的模型
  ollama list
  # 删除指定模型（例如删除phi3）
  ollama rm phi3
  ```

## 常见问题排查
### Q1: 启动时报“Ollama未启动或连接失败”
- 确认 `ollama serve` 已启动（单独终端窗口运行，不可关闭）
- 检查 `config/settings.py` 中 `OLLAMA_HOST = "http://127.0.0.1:11434"` 配置正确
- 重启Ollama服务：先执行 `ollama stop`，再重新执行 `ollama serve`
- 检查防火墙是否拦截Ollama端口（默认11434）

### Q2: 上传文档提示“上传失败”
- 文档格式：仅支持PDF/TXT，其他格式（如docx）需先转换为TXT/PDF
- 文档编码：TXT文档建议使用UTF-8编码（可通过记事本另存为选择编码）
- 权限问题：确保项目目录有读写权限（右键项目文件夹 → 属性 → 安全 → 允许写入）
- 文档大小：单文件建议≤100MB，超大文件可拆分后上传

### Q3: 提示“所选模型未全部下载，无法上传文件/问答”
- 原因：选择的模型未下载到本地，系统拦截了无效操作
- 解决方案：
  1. 复制界面提示的下载命令到终端执行（如 `ollama pull phi3`）
  2. 等待模型下载完成后，重新点击「初始化模型」
  3. 确认模型状态提示“所有模型已就绪”后再操作

### Q4: 回复内容乱码/不完整
- 文档分块优化：调整 `config/settings.py` 中 `CHUNK_SIZE`（默认1000，可改为500/1500）
- 提示词优化：修改 `ui/gradio_ui.py` 中 `qa_prompt` 自定义回复规则
- 模型选择：切换为中文优化模型（如qwen）可提升中文回复质量

### Q5: 内存不足，无法下载多个模型怎么办？
- 选择**小体积模型组合**（如 `phi3 + all-minilm`，总占用约2.1GB）
- 用完不常用的模型后，执行 `ollama rm 模型名` 删除，释放内存
- 无需提前下载所有模型，按需下载即可，切换模型时再下载对应版本

## 自定义配置
### 修改模型（多模型切换核心）
编辑 `config/model_config.py`，新增/修改支持的模型列表：
```python
# 支持的大语言模型（可自行扩展）
SUPPORTED_LLM_MODELS = {
    "llama3": "Meta Llama3（推荐）",
    "qwen": "阿里云通义千问（中文优化）",
    "phi3": "Microsoft Phi3（小体积）",
    "gemma": "Google Gemma"
}

# 支持的嵌入模型
SUPPORTED_EMBEDDING_MODELS = {
    "nomic-embed-text": "Nomic Embed Text（推荐）",
    "bge-large": "BGE Large（高精度）",
    "all-minilm": "All MiniLM（超轻量）"
}
```

### 修改端口（避免端口冲突）
编辑 `config/settings.py`：
```python
# 避免端口冲突，可改为7861/8080等
GRADIO_PORT = 7861
```

### 调整文档分块（优化检索效果）
编辑 `config/settings.py`：
```python
# 分块大小（字符数），默认1000，可根据文档类型调整
CHUNK_SIZE = 1500
# 分块重叠（避免上下文断裂），默认100
CHUNK_OVERLAP = 150
```

## 免责声明
- 本工具仅用于学习研究和企业内部自用，请勿用于商业用途
- 模型回复仅供参考，准确性取决于上传文档的质量和模型的能力
- 请确保所上传的文档拥有合法使用权限，禁止上传涉密、侵权等违规文档
- 作者不对使用本工具产生的任何后果承担责任